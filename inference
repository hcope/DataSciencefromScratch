#chapter 7: hypothesis and inference
import probability tools
import math

def normal_approxiation_to_binomial(n, p):
    """
    Recll that a repeated Bernoulli trial creates a binomial rancom variable, which can be approx. as normal distro
    """
    mu = n * p
    sigma = math.sqrt(p * (1 - p) * n)
    return mu, sigma

#in this section, we'll be figuring our the probability a random variable's realized value is in/out side an interval

normal_probability_below = normal_cdf

def normal_probability_above(lo, mu=0, sigma=1):
    return 1 - normal_cdf(lo, mu, sigma)
    
def normal_probability_between(lo, hi, mu=0, sigma=1):
    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)

def normal_probability_outside(lo, hi, mu=0, sigma=1):
    return 1 - normal_probability_between(lo, hi, mu, sigma)
    
#find the interval centered around the mean that contains some lebel of likelihood

normal_upper_bound = inverse_normal_cdf

def normal_lower_bound(probability, mu=0, sigma=1):
    return inverse_normal_cdf(1 - probability, mu=0, sigma=1)

def normal_two_sided_bounds(probability, mu=0, sigma=1):
    tailProbability = (1 - probability)/2.0
    upperBound = normal_lower_bound(tail_probability, mu, sigma)
    lowerBound = normal_upper_bound(tail_probability, mu, sigma)
    return lowerBound, upperBound
    
#when we want to test significance:
#type 1 error is a false positive, in which we reject H_0 even if true. Willing to make this error 1 or 5 percent of time
#when we want to know about the power of a test:
#type 2 error is failing to regect H_0 even though it's false. Need to be specific about what H_0 means

def two_sided_p_value(x, mu=0, sigma=1):
    """
    compute the probability that, given H_0 is true, that we see an outcome like we observed.
    """
    if x >= mu:
        return 2 * normal_probability_above(x, mu, sigma)
    else:
        return 2 * normal_probability_below(x, mu, sigma)

#if this value is smaller than our significance, we regect H_0
#this can then lead us to the idea of confidence.

#say we're given a binary variable that occurs pHat percent of the time when we observe it in 1000 trials
#note that this is *not* the same as knowing the probability it occurs
numTrials = 1000.
pHat = 0.52
mu = pHat
sigma = math.sqrt(pHat * (1 - pHat) / numTrials)
#the above is common but not entiredly justified

#if we do the following, we say that we are 95 percent confident that the true value occurs within the bounds
normal_two_sided_bounds(0.95, mu, sigma)

#note that we can easily mess with these values if we remove specific parts of the data, like outliers and stuff

#bayesian inference
#start with a prior belief distribution, uses observed data and Baye's thm to update

def B(alpha, beta):
    """
    Normalizing function. Makes sure total prob. is 1
    """
    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha+beta)

def beta_pdf(x, alpha, beta):
    if x < 0 or x> 1:
        return 0
    return x**(alpha - 1) * (1 - x) ** (beta - 1)/B(alpha, beta)

#note that there are some philosophical issues with choosing a prior and then seeing what happens
